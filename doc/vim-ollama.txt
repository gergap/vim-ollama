" vim-ollama.txt   For Vim version 8.0 or later  Last change: 2025-03-18

*vim-ollama*

                     VIM-OLLAMA
                 by Gerhard Gappmeier
                 gergap/vim-ollama

==============================================================================
CONTENTS:
    1. Introduction                       |vim-ollama-intro|
    2. Installation                       |vim-ollama-install|
    3. Usage                              |vim-ollama-usage|
    4. Commands                           |vim-ollama-commands|
    5. Maps                               |vim-ollama-maps|
    6. Configuration                      |vim-ollama-config|
    7. Troubleshooting                    |vim-ollama-troubleshooting|
    8. License                            |vim-ollama-license|

==============================================================================
1. Introduction                                       *vim-ollama-intro*

vim-ollama is a Vim plugin that integrates with the Ollama engine. It allows
you to interact with various LLM models directly from the Vim editor,
enhancing your workflow with AI capabilities.

Ollama can be run locally, no cloud is required, thus preserving your privacy.
Ollama is Open Source (MIT license) so you get the source code and it is free
to use.

==============================================================================
2. Installation                                       *vim-ollama-install*

To install vim-ollama, you can use any Vim plugin manager like vim-plug or
Vundle.

With vim-plug:
>
    Plug 'gergap/vim-ollama'
<

After adding the line above to your .vimrc:
>
    :source %
    :PlugInstall
<
==============================================================================
3. Usage                                              *vim-ollama-usage*

Once installed, you can start using vim-ollama commands within Vim. The plugin
will display completions automatically as ghost text. Hit the <Tab> key to
accept the completion suggestion.

You can also visually select parts of your code and use :OllamaEdit or the key
`<leader>e` to start editing this code by using a LLM prompt. Is can be useful e.g.
for adding comments, translating text, refactoring code and much more.

==============================================================================
4. Commands                                           *vim-ollama-commands*

The vim-ollama plugin provides the following commands:

                                                      *:Ollama*
:Ollama
    - Description: Sends commands to the plugin to change its state.
    Available commands are:
      - `setup`: Starts the Vim-Ollama setup wizard.
      - `config`: Opens the Vim-Ollama default config file.
      - `enable`: Enables AI tab completions.
      - `disable`: Disables AI tab completions.
      - `toggle`: Toggles the enabled state of the plugin.
      - `pipinstall`: Installs all python dependencies in a Vim-Ollama
        specific virtual environment. This is done automatically during
        the setup wizard, but this command may be useful when updating
        the plugin and the python requirements have changed.

    - Usage:
>
        :Ollama [command]
<
    - Example:
>
        :Ollama disable
<
                                                      *:OllamaChat*
:OllamaChat
    - Description: Creates a split windows for interactive conversations with
    the configured chat model. Use `:bd` to delete the chat buffer when you
    don't need anymore.

                                                      *:OllamaReview*
:OllamaReview
    - Description: Reviews the selected text. It opens a chat window like
    OllamaChat, but with a predefined prompt that asks for a code review of
    the selected text. The selection may be a few lines, a function or the
    whole file (`:%OllamaRewiew`).

    - Usage:
>
        :OllamaReview
<
                                                      *:OllamaSpellCheck*
:OllamaSpellCheck
    - Description: Checks the selected text for spelling errors.
    Like OllamaReview this command using the OllamaChat window but with a
    predefined prompt for spell checking. The selection may be a few lines, a
    paragraph or the whole file (`:%OllamaSpellCheck`).

    - Usage:
>
        :OllamaSpellCheck
<
                                                      *:OllamaTask*
:OllamaTask
    - Description: Creates a chat window with a custom prompt. This command
    also works on a range like `:OllamaReview` but takes an additional
    argument for the prompt. The selected range will automatically pasted
    into the chat buffer.
    - Usage:
>
        :OllamaTask [prompt]
<
    - Example:
>
        :OllamaTask "check spelling of this text"
<
                                                      *:OllamaEdit*
:OllamaEdit
    - Description: Allows editing a selected text using the AI. This command
    works on a range and so can use the whole file `:%OllamaEdit <prompt>` or
    a visual selection. Alternatively, you can use the mapping `<leader>e` to
    achieve the same, but the command has the advantage of a command history,
    which may be handy when you want to apply the same change to multiple
    locations.
    - Usage:
>
        :OllamaEdit [prompt]
<
    - Example:
>
        :OllamaEdit "add comments to this code"
<
    - Accepting and Rejecting changes: The changes made by the AI are applied
    as an inline diff. You can accept/reject each individual change or all.
    The dialog uses Vim's `popup_filter_yesno`, which means you can accept
    using the keys 'y', 'Y' and reject using 'n' or 'N'. Pressing Esc and 'x'
    works like pressing 'n'. `

                                                      *:OllamaPull*
:OllamaPull
    - Description: Pulls a new model. This works like `ollama pull` command
    but via the REST API and the configured `g:ollama_host` URL, so you can use
    this to pull new models on remote machines, without needing to SSH in to
    this machine.
    It uses the existing 'pull' function that was created for the setup wizard.
    - Usage:
>
        :OllamaPull [modelname]
<
    - Example:
>
        :OllamaPull qwen2.5-coder:1.5b
<

==============================================================================
5. Maps                                               *vim-ollama-maps*

                                                      *vim-ollama-i_<Tab>*
Vim-ollama uses <Tab> to accept the current suggestion.  If you have an
existing <Tab> map, that will be used as the fallback when no suggestion is
displayed.

<M-Right>               Inserts only the next line of the current suggestion.
<Plug>(ollama-insert-line)

<M-C-Right>             Inserts only the next word of the current suggestion.
<Plug>(ollama-insert-word)

Other Maps ~

                                                       *vim-ollama-leader_r*
<leader>r               Calls ollama#Review() to review the current selection.
<Plug>(ollama-review)

<C-]>                   Dismiss the current suggestion.
<Plug>(ollama-dismiss)

<leader>e               Starts editing the current selection or the whole file
                        if there is not visual selection.
<Plug>(ollama-edit)

<C-Y>                   Accept all changes.
<Plug>(ollama-accept-all-changes)

<C-N>                   Reject all changes.
<Plug>(ollama-reject-all-changes)

==============================================================================
6. Configuration                                      *vim-ollama-config*

You can configure vim-ollama using the following variables in your .vimrc:

                                                      *g:ollama_enabled*
g:ollama_enabled
    - Description: When set to `1` (or 'v:true') enables vim-ollama
      autocompletion for all buffers, by default.  But also see
      `b:ollama_enabled`.  If set to any other value, vim-ollama
      autocompletion will be disabled by default for all buffers.

      Can be modified either directly, or via the `:Ollama enable`,
      `:Ollama disable`, and `:Ollama toggle` commands.
    - Default: 'v:true'
    - Example:
>
        let g:ollama_enabled = 0
<
                                                      *b:ollama_enabled*
b:ollama_enabled
    - Description: Similar to `g:ollama_enabled`, but affects a single buffer.
      When unset, `g:ollama_enabled` is used.

      If you want to disable vim-ollama autocompletion by default, but enable
      it for individual buffers, you can set `g:ollama_enabled` to 'v:false',
      and use an 'autocmd' to set `b:ollama_enabled` to 'v:true' for
      individual buffers.

      Or vice versa, you can set `g:ollama_enabled` to 'v:true', and use an
      'autocmd' to set `b:ollama_enabled` to 'v:false' for individual buffers.

    - Default: unset
    - Example:
>
        " Disable vim-ollama autocompletion for a specific filetype
        augroup DisableOllamaAutoCompletionForPerl
          autocmd!
          autocmd FileType perl let b:ollama_enabled = v:false
        augroup END

        " Enable vim-ollama autocompletion only for a specific filetype
        let g:ollama_enabled = v:false
        augroup EnableOllamaAutoCompletionForPerl
          autocmd!
          autocmd FileType perl let b:ollama_enabled = v:true
        augroup END
<
                                                      *g:ollama_host*
g:ollama_host
    - Description: Sets the Ollama host address and port.
    - Default: 'http://localhost:11434'
    - Example:
>
        let g:ollama_host = 'http://your-server:port'
<
                                                      *g:ollama_openai_baseurl*
g:ollama_openai_baseurl
    - Description: Sets the OpenAI base URL used for OpenAI backend.
    - Default: ''
    - Example:
>
        let g:ollama_openai_baseurl = 'http://localhost:1234'
<
                                                      *g:ollama_openai_credentialname*
g:ollama_openai_credentialname
    - Description: Sets the credentialname for loading the API-key from the
      UNIX password store. This avoids setting the OPENAI_API_KEY or MISTRAL_API_KEY
      as environment variable.
    - Default: ''
    - Example:
>
        let g:ollama_openai_credentialname = 'api-tokens/mistral-ai'
<
                                                      *g:ollama_model_provider*
g:ollama_model_provider
    - Description: Configures the REST API backend for code completion.
      Currently, 'ollama', 'openai' and 'openai_legacy' are supported.
      OpenAI notes: The OpenAI '/v1/chat/completions' endpoint does not support
      fill-in-the-middle, which is required for good code completion. The
      plugin tries to work around this using prompt engineering. Choose
      'ollama' or 'openai_legacy' ('/v1/completions' endpoint) to make
      fill-in-the-middle working.
    - Default: 'ollama'
    - Example:
>
        let g:ollama_model_provider = 'openai_legacy'
<
                                                      *g:ollama_model*
g:ollama_model
    - Description: Sets the default Ollama model for code completions.
    - Default: 'codellama:code'
    - Example:
>
        let g:ollama_model = 'deepseek-coder-v2:16b-lite-base-q4_0'
<
                                                      *g:ollama_model_options*
g:ollama_model_options
    - Description: Sets the default Ollama model options.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_model_options = {
                \ 'temperature': 0,
                \ 'top_p': 0.95,
                \ 'num_predict': 256
                \ }
<
                                                      *g:ollama_context_lines*
g:ollama_context_lines
    - Description: Sets the number of context lines before and after the
      cursor to be transmitted to the LLM for tab completion.
      When running a LLM locally on CPU (without GPU support), reduce this
      value to speed up completion. Small values like 2 already work well,
      for simple tasks.
    - Default: 30
>
        let g:ollama_context_lines = 10
<
                                                      *g:ollama_debounce_time*
g:ollama_debounce_time
    - Description: Sets the delay after the last keystroke before triggering
      a new search. Using smaller values give you faster reaction times, but
      create higher load on the system. Small values make only sense on fast
      GPU based LLMs, where the LLM computation time is not much longer than
      the debounce time.
      By settings this to 0 you can disable the auto-trigger behavior and
      define a mapping for `<Plug>(ollama-trigger-completion)` instead.
    - Default: 500 ms
    - Example:
>
        let g:ollama_debounce_time = 300
<
                                      *g:ollama_completion_allowlist_filetype*
g:ollama_completion_allowlist_filetype
    - Description: If non-empty, then completion will only be triggered for
      buffers with these file types (See 'filetype').

      You should set this, if you prefer completion to be disabled for any
      file type, except the ones that you listed. You can still trigger
      completion with the `<Plug>(ollama-trigger-completion)` mapping.

      File type is checked first against this list, then checked against the
      `g:ollama_completion_denylist_filetype`.  So if both match, completion
      will be disabled for this buffer.

    - Default: []
    - Example:
>
        let g:ollama_completion_allowlist_filetype = ['cpp', 'haskell']
<
                                        *g:ollama_completion_denylist_filetype*
g:ollama_completion_denylist_filetype
    - Description: If non-empty, then completion will not be triggered for
      buffers with these file types (See 'filetype').

      This only affects the automatic completion suggestion, triggered by
      `g:ollama_debounce_time`. You can always trigger completion manually
      with the `<Plug>(ollama-trigger-completion)` mapping.

      It probably makes sense to disable completion, for example, in the
      fuzzyfinder plugin window.  But depending on the other plugins you have
      available, you may want to add other filetypes as well.

      This list is checked after `g:ollama_completion_allowlist_filetype`.  So
      if both match, completion will be disabled for this buffer.

    - Default: []
    - Example:
>
        let g:ollama_completion_denylist_filetype = ['fuf']
<
                                                      *g:ollama_chat_provider*
g:ollama_chat_provider
    - Description: Configures the REST API backend for chat conversations.
      Currently, 'ollama' and 'openai' are supported.
    - Default: 'ollama'
    - Example:
>
        let g:ollama_chat_provider = 'openai'
<
                                                      *g:ollama_chat_model*
g:ollama_chat_model
    - Description: Sets the default Ollama model to use for interactive chats.
    - Default: 'llama3'
    - Example:
>
        let g:ollama_chat_model = 'gpt4-turbo'
<
                                                  *g:ollama_chat_systemprompt*
g:ollama_chat_systemprompt
    - Description: Allows overriding the system prompt of the chat model. If
      not specified the models default system prompt will be used.
    - Default: ''
    - Example:
>
        let g:ollama_chat_systemprompt = 'You are a coding assistant. Output
        only code, no explanations.'
<
                                                      *g:ollama_chat_options*
g:ollama_chat_options
    - Description: Sets the default Ollama model options for chat models.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_chat_options = {
                \ 'temperature': 0.2,
                \ 'top_p': 0.95
                \ }
<
                                                      *g:ollama_chat_timeout*
g:ollama_chat_timeout
    - Description: Sets the timeout value in chat requests.
    - Default: 10
    - Example:
>
        let g:ollama_chat_timeout = 10
<
                                                      *g:ollama_edit_provider*
g:ollama_edit_provider
    - Description: Configures the REST API backend for edit tasks.
      Currently, 'ollama' and 'openai' are supported.
    - Default: 'ollama'
    - Example:
>
        let g:ollama_edit_provider = 'openai'
<
                                                      *g:ollama_edit_model*
g:ollama_edit_model
    - Description: Sets the default Ollama model to use for code editing.
    - Default: 'qwen2.5-coder:7b'
    - Example:
>
        let g:ollama_chat_model = 'qwen2.5-coder:7b'
<
                                                      *g:ollama_edit_options*
g:ollama_edit_options
    - Description: Sets the default Ollama model options for code edit models.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_edit_options = {
                \ 'temperature': 0,
                \ 'top_p': 0.95,
                \ 'num_predict': 256
                \ }
<
                                                      *g:ollama_use_inline_diff*
g:ollama_use_inline_diff
    - Description: When true, the `OllamaEdit` changes are applied as inline
      diff which can be accepted or rejected individually in an interactive
      way. When false, the plugin will apply the changes directly without
      asking. This may be useful when tracking the code via Git. Using
      vim-fugitive's `:GVdiffsplit` command you can compare the changes
      in vimdiff mode.
    - Default: 1
    - Example:
>
        let g:ollama_use_inline_diff = 0
<
                                                      *g:ollama_no_maps*
g:ollama_no_maps
    - Description: Disables all default mappings except for `<tab>` which has
      a built-in fallback mechanism. This can be useful to avoid conflicts
      with other plugins and to define your own mappings for vim-ollama.
    - Default: 0
    - Example:
>
        let g:ollama_no_maps = 1
<
                                                      *g:ollama_no_tab_map*
g:ollama_no_tab_map
    - Description: If this variable is defined, the default <tab> mapping
      will not be created.
    - Default: undefined
    - Example:
>
        let g:ollama_no_tab_map = v:true
        inoremap ,a <Plug>(ollama-tab-completion)
        inoremap ,w <Plug>(ollama-insert-word)
        inoremap ,l <Plug>(ollama-insert-line)
<
                                                      *g:ollama_use_venv*
g:ollama_use_venv
    - Description: Enables the usage of python virtual environments for
      executing the required python scripts. If enabled it will install
      the required packages automatically in this vim-ollama specific
      virtual environment.
      When disabled, you an still manually install the packges as before.
    - Default: 0 (if missing), but setup wizard creates a config with 1 by default.
    - Example:
>
        let g:ollama_use_venv = 1
<
                                                    *g:ollama_split_vertically*
g:ollama_split_vertically
    - Description: When this variable is defined and set to 1 the OllamaChat
      command will create a vertical split, otherwise a horizontal split is
      used.
    - Default: 1
>
        " prefer horizontal split
        let g:ollama_split_vertically = 0
<

==============================================================================
7. Troubleshooting                                    *vim-ollama-troubleshooting*

You can enable logging by specifying the following variables.

                                                      *g:ollama_debug*
g:ollama_debug
    - Description: Sets the debug level of the logging infrastructure.
    - Levels: 0 (Off), 1 (Errors), 2 (Warnings), 3 (Info), 4 (Debug)
      Higher levels also include all lower levels.
    - Default: 0 (Off)
    - Example:
>
        let g:ollama_debug = 4
<
                                                      *g:ollama_logfile*
g:ollama_logfile
    - Description: Configure the path of the log file.
      Note that the log file path of the python code is currently hardcoded to
      '/tmp/logs'.
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama.log'
    - Example:
>
        let g:ollama_logfile = '/path/to/vim-ollama.log'
<
                                                      *g:ollama_review_logfile*
g:ollama_review_logfile
    - Description: Configure the path for logging chat conversations.
      This only will be logged on _Debug_ level (4).
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama-review.log'
    - Example:
>
        let g:ollama_review_logfile = '/path/to/vim-ollama-review.log'

By default, the Python scripts used underneath use separate log files. The
variable `g:ollama_debug` is passed to the Python calls to enable also the
logging mechanism of the according script. However, the default log path
is `/tmp/logs`, and the log filenames are `complete.log` for tab completions
and `chat.log` for chat conversations.

==============================================================================
8. License                                            *vim-ollama-license*

vim-ollama is open-source software licensed under the GPLv3 License. For more
information, see the LICENSE file in the repository.

==============================================================================
vim:tw=78:et:ft=help:norl:

