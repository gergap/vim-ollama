" vim-ollama.txt   For Vim version 8.0 or later  Last change: 2025-02-08

*vim-ollama*

                     VIM-OLLAMA
                 by Gerhard Gappmeier
                 gergap/vim-ollama

==============================================================================
CONTENTS:
    1. Introduction                       |vim-ollama-intro|
    2. Installation                       |vim-ollama-install|
    3. Usage                              |vim-ollama-usage|
    4. Commands                           |vim-ollama-commands|
    5. Maps                               |vim-ollama-maps|
    6. Configuration                      |vim-ollama-config|
    7. Troubleshooting                    |vim-ollama-troubleshooting|
    8. License                            |vim-ollama-license|

==============================================================================
1. Introduction                                       *vim-ollama-intro*

vim-ollama is a Vim plugin that integrates with the Ollama engine. It allows
you to interact with various LLM models directly from the Vim editor,
enhancing your workflow with AI capabilities.

Ollama can be run locally, no cloud is required, thus preserving your privacy.
Ollama is Open Source (MIT license) so you get the source code and it is free
to use.

==============================================================================
2. Installation                                       *vim-ollama-install*

To install vim-ollama, you can use any Vim plugin manager like vim-plug or
Vundle.

With vim-plug:
>
    Plug 'gergap/vim-ollama'
<

After adding the line above to your .vimrc:
>
    :source %
    :PlugInstall
<
==============================================================================
3. Usage                                              *vim-ollama-usage*

Once installed, you can start using vim-ollama commands within Vim. The plugin
will display completions automatically as ghost text. Hit the <Tab> key to
accept the completion suggestion.

You can also visually select parts of your code and use :OllamaEdit or the key
<C-I> to start editing this code by using a LLM prompt. Is can be useful e.g.
for adding comments, translating text, refactoring code and much more.

==============================================================================
4. Commands                                           *vim-ollama-commands*

The vim-ollama plugin provides the following commands:

1. :Ollama
    - Description: Sends commands to the plugin to change its state.
    Available commands are:
      - `setup`: Starts the Vim-Ollama setup wizard.
      - `config`: Opens the Vim-Ollama default config file.
      - `enable`: Enables AI tab completions.
      - `disable`: Disables AI tab completions.
      - `toggle`: Toggles the enabled state of the plugin.

    - Usage:
>
        :Ollama [command]
<
    - Example:
>
        :Ollama disable
<
2. :OllamaChat
    - Description: Creates a split windows for interactive conversations with
    the configured chat model. Use `:bd` to delete the chat buffer when you
    don't need anymore.

3. :OllamaReview
    - Description: Reviews the selected text. It opens a chat window like
    OllamaChat, but with a predefined prompt that asks for a code review of
    the selected text. The selection may be a few lines, a function or the
    whole file (`:%OllamaRewiew`).

    - Usage:
>
        :OllamaReview
<
4. :OllamaSpellCheck
    - Description: Checks the selected text for spelling errors.
    Like OllamaReview this command using the OllamaChat window but with a
    predefined prompt for spell checking. The selection may be a few lines, a
    paragraph or the whole file (`:%OllamaSpellCheck`).

    - Usage:
>
        :OllamaSpellCheck
<
5. :OllamaTask
    - Description: Creates a chat window with a custom prompt. This command
    also works on a range like `:OllamaReview` but takes an additional
    argument for the prompt. The selected range will automatically pasted
    into the chat buffer.
    - Usage:
>
        :OllamaTask [prompt]
<
    - Example:
>
        :OllamaTask "check spelling of this text"
<
6. :OllamaEdit
    - Description: Allows editing a selected text using the AI. This command
    works on a range and so can use the whole file `:%OllamaEdit <prompt>` or
    a visual selection. Alternatively, you can use the mapping `<C-I>` to
    achieve the same, but the command has the advantage of a command history,
    which may be handy when you want to apply the same change to multiple
    locations.
    - Usage:
>
        :OllamaEdit [prompt]
<
    - Example:
>
        :OllamaEdit "add comments to this code"
<
    - Accepting and Rejecting changes: The changes made by the AI are applied
    as an inline diff. You can accept/reject each individual change or all.
    The dialog uses Vim's `popup_filter_yesno`, which means you can accept
    using the keys 'y', 'Y' and reject using 'n' or 'N'. Pressing Esc and 'x'
    works like pressing 'n'. `

==============================================================================
5. Maps                                               *vim-ollama-maps*

                                                      *vim-ollama-i_<Tab>*
Vim-ollama uses <Tab> to accept the current suggestion.  If you have an
existing <Tab> map, that will be used as the fallback when no suggestion is
displayed.

<M-Right>               Inserts only the next line of the current suggestion.
<Plug>(ollama-insert-line)

<M-C-Right>             Inserts only the next word of the current suggestion.
<Plug>(ollama-insert-word)

Other Maps ~

                                                       *vim-ollama-leader_r*
<leader>r               Calls ollama#Review() to review the current selection.
<Plug>(ollama-review)

<C-]>                   Dismiss the current suggestion.
<Plug>(ollama-dismiss)

<C-I>                   Starts editing the current selection or the whole file
                        if there is not visual selection.
<Plug>(ollama-edit)

<C-Y>                   Accept all changes.
<Plug>(ollama-accept-all-changes)

<C-N>                   Reject all changes.
<Plug>(ollama-reject-all-changes)

==============================================================================
6. Configuration                                      *vim-ollama-config*

You can configure vim-ollama using the following variables in your .vimrc:

1. g:ollama_host
    - Description: Sets the Ollama host address and port.
    - Default: 'http://localhost:11434'
    - Example:
>
        let g:ollama_host = 'http://your-server:port'
<
2. g:ollama_model
    - Description: Sets the default Ollama model for code completions.
    - Default: 'codellama:code'
    - Example:
>
        let g:ollama_model = 'deepseek-coder-v2:16b-lite-base-q4_0'
<
3. g:ollama_model_options
    - Description: Sets the default Ollama model options.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_model_options = {
                \ 'temperature': 0,
                \ 'top_p': 0.95,
                \ 'num_predict': 256
                \ }
<
4. g:ollama_context_lines
    - Description: Sets the number of context lines before and after the
      cursor to be transmitted to the LLM for tab completion.
      When running a LLM locally on CPU (without GPU support), reduce this
      value to speed up completion. Small values like 2 already work well,
      for simple tasks.
    - Default: 30
>
        let g:ollama_context_lines = 10
<
5. g:ollama_debounce_time
    - Description: Sets the delay after the last keystroke before triggering
      a new search. Using smaller values give you faster reaction times, but
      create higher load on the system. Small values make only sense on fast
      GPU based LLMs, where the LLM computation time is not much longer than
      the debounce time.
    - Default: 500 ms
    - Example:
>
        let g:ollama_debounce_time = 300
<
6. g:ollama_chat_model
    - Description: Sets the default Ollama model to use for interactive chats.
    - Default: 'llama3'
    - Example:
>
        let g:ollama_chat_model = 'gpt4-turbo'
<
7. g:ollama_chat_systemprompt
    - Description: Allows overriding the system prompt of the chat model.
    - Default: ''. If not specified the built-in prompt will be used.
    - Example:
>
        let g:ollama_chat_systemprompt = 'You are a coding assistant. Output
        only code, no explanations.'
<
8. g:ollama_chat_options
    - Description: Sets the default Ollama model options for chat models.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_chat_options = {
                \ 'temperature': 0.2,
                \ 'top_p': 0.95
                \ }
<
9. g:ollama_chat_timeout
    - Description: Sets the timeout value in chat requests.
    - Default: 10
    - Example:
>
        let g:ollama_chat_timeout = 10
<
10. g:ollama_edit_model
    - Description: Sets the default Ollama model to use for code editing.
    - Default: 'qwen2.5-coder:7b'
    - Example:
>
        let g:ollama_chat_model = 'qwen2.5-coder:7b'
<
11. g:ollama_edit_options
    - Description: Sets the default Ollama model options for code edit models.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_edit_options = {
                \ 'temperature': 0,
                \ 'top_p': 0.95,
                \ 'num_predict': 256
                \ }
<
12. g:ollama_use_inline_diff
    - Description: When true, the `OllamaEdit` changes are applied as inline
      diff which can be accepted or rejected individually in an interactive
      way. When false, the plugin will apply the changes directly without
      asking. This may be useful when tracking the code via Git. Using
      vim-fugitive's `:GVdiffsplit` command you can compare the changes
      in vimdiff mode.
    - Default: 1
    - Example:
>
        let g:ollama_use_inline_diff = 0
<
13. g:ollama_no_maps
    - Description: Disables all default mappings except for `<tab>` which has
      a built-in fallback mechanism. This can be useful to avoid conflicts
      with other plugins and to define your own mappings for vim-ollama.
    - Default: 0
    - Example:
>
        let g:ollama_no_maps = 1
<

13. g:ollama_unmap_tab
    - Description: Variable, which if set to anything, will unmap tab as the
      suggestion insertion key.
    - Default: undefined
    - Example:
>
        let g:ollama_unmap_tab = v:true
        inoremap ,- <Plug>(ollama-insert-word)
        inoremap ,c <Plug>(ollama-insert-line)

==============================================================================
7. Troubleshooting                                    *vim-ollama-troubleshooting*

You can enable logging by specifying the following variables.

1. g:ollama_debug
    - Description: Sets the debug level of the logging infrastructure.
    - Levels: 0 (Off), 1 (Errors), 2 (Warnings), 3 (Info), 4 (Debug)
      Higher levels also include all lower levels.
    - Default: 0 (Off)
    - Example:
>
        let g:ollama_debug = 4

2. g:ollama_logfile
    - Description: Configure the path of the log file.
      Note that the log file path of the python code is currently hardcoded to
      '/tmp/logs'.
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama.log'
    - Example:
>
        let g:ollama_logfile = '/path/to/vim-ollama.log'

3. g:ollama_review_logfile
    - Description: Configure the path for logging chat conversations.
      This only will be logged on _Debug_ level (4).
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama-review.log'
    - Example:
>
        let g:ollama_review_logfile = '/path/to/vim-ollama-review.log'

By default, the Python scripts used underneath use separate log files. The
variable `g:ollama_debug` is passed to the Python calls to enable also the
logging mechanism of the according script. However, the default log path
is `/tmp/logs`, and the log filenames are `complete.log` for tab completions
and `chat.log` for chat conversations.

==============================================================================
8. License                                            *vim-ollama-license*

vim-ollama is open-source software licensed under the GPLv3 License. For more
information, see the LICENSE file in the repository.

==============================================================================
vim:tw=78:et:ft=help:norl:

