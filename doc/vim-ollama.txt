" vim-ollama.txt   For Vim version 8.0 or later  Last change: 2024-09-22

*vim-ollama*

                     VIM-OLLAMA
                 by Gerhard Gappmeier
                 gergap/vim-ollama

==============================================================================
CONTENTS:
    1. Introduction                       |vim-ollama-intro|
    2. Installation                       |vim-ollama-install|
    3. Usage                              |vim-ollama-usage|
    4. Commands                           |vim-ollama-commands|
    5. Maps                               |vim-ollama-maps|
    6. Configuration                      |vim-ollama-config|
    7. Troubleshooting                    |vim-ollama-troubleshooting|
    8. License                            |vim-ollama-license|

==============================================================================
1. Introduction                                       *vim-ollama-intro*

vim-ollama is a Vim plugin that integrates with the Ollama engine. It allows
you to interact with various LLM models directly from the Vim editor,
enhancing your workflow with AI capabilities.

Ollama can be run locally, no cloud is required, thus preserving your privacy.
Ollama is Open Source (MIT license) so you get the source code and it is free
to use.

==============================================================================
2. Installation                                       *vim-ollama-install*

To install vim-ollama, you can use any Vim plugin manager like vim-plug or
Vundle.

With vim-plug:
>
    Plug 'gergap/vim-ollama'
<

After adding the line above to your .vimrc:
>
    :source %
    :PlugInstall
<
==============================================================================
3. Usage                                              *vim-ollama-usage*

Once installed, you can start using vim-ollama commands within Vim. The plugin
will display completions automatically as ghost text. Hit the <Tab> key to
accept the completion suggestion.

==============================================================================
4. Commands                                           *vim-ollama-commands*

The vim-ollama plugin provides the following commands:

1. :Ollama
    - Description: Sends commands to the plugin to change its state.
    Available commands are `enable`, `disable` and `toggle`.

    - Usage:
>
        :Ollama [command]
<
    - Example:
>
        :Ollama disable
<
2. :OllamaChat
    - Description: Creates a split windows for interactive conversations with
    the configured chat model. Use `:bd` to delete the chat buffer when you
    don't need anymore.

3. :OllamaReview
    - Description: Reviews the selected text. It opens a chat window like
    OllamaChat, but with a predefined prompt that asks for a code review of
    the selected text. The selection may be a few lines, a function or the
    whole file (`:%OllamaRewiew`).

    - Usage:
>
        :OllamaReview
<
4. :OllamaSpellCheck
    - Description: Checks the selected text for spelling errors.
    Like OllamaReview this command using the OllamaChat window but with a
    predefined prompt for spell checking. The selection may be a few lines, a
    paragraph or the whole file (`:%OllamaSpellCheck`).

    - Usage:
>
        :OllamaSpellCheck
<
5. :OllamaTask
    - Description: Creates a chat window with a custom prompt. This command
    also works on a range like `:OllamaReview` but takes an additional
    argument for the prompt. The selected range will automatically pasted
    into the chat buffer.
    - Usage:
>
        :OllamaTask [prompt]
<
    - Example:
>
        :OllamaTask "check spelling of this text"
<
==============================================================================
5. Maps                                               *vim-ollama-maps*

                                                      *vim-ollama-i_<Tab>*
Vim-ollama uses <Tab> to accept the current suggestion.  If you have an
existing <Tab> map, that will be used as the fallback when no suggestion is
displayed.

<M-Right>               Inserts only the next line of the current suggestion.
<Plug>(ollama-insert-line)

<M-C-Right>             Inserts only the next word of the current suggestion.
<Plug>(ollama-insert-word)

Other Maps ~

                                                       *vim-ollama-leader_r*
<leader>r               Calls ollama#Review() to review the current selection.
<Plug>(ollama-review)

<C-]>                   Dismiss the current suggestion.
<Plug>(ollama-dismiss)

==============================================================================
6. Configuration                                      *vim-ollama-config*

You can configure vim-ollama using the following variables in your .vimrc:

1. g:ollama_host
    - Description: Sets the Ollama host address and port.
    - Default: 'http://localhost:11434'
    - Example:
>
        let g:ollama_host = 'http://your-server:port'
<
2. g:ollama_chat_model
    - Description: Sets the default Ollama model to use for interactive chats.
    - Default: 'llama3'
    - Example:
>
        let g:ollama_chat_model = 'gpt4-turbo'
<
3. g:ollama_model
    - Description: Sets the default Ollama model for code completions.
    - Default: 'codellama:code'
    - Example:
>
        let g:ollama_model = 'deepseek-coder-v2:16b-lite-base-q4_0'

4. g:ollama_timeout
    - Description: Sets the timeout value in chat requests.
    - Default: 10
    - Example:
>
        let g:ollama_timeout = 10
<
5. g:ollama_model_options
    - Description: Sets the default Ollama model options.
    - Default: '{"temperature": 0, "top_p": 0.95}'
    - Example:
>
        let g:ollama_model_options = {
                \ 'temperature': 0,
                \ 'top_p': 0.95,
                \ 'num_predict': 256
                \ }
<
==============================================================================
7. Troubleshooting                                    *vim-ollama-troubleshooting*

You can enable logging by specifying the following variables.

1. g:ollama_debug
    - Description: Sets the debug level of the logging infrastructure.
    - Levels: 0 (Off), 1 (Errors), 2 (Warnings), 3 (Info), 4 (Debug)
      Higher levels also include all lower levels.
    - Default: 0 (Off)
    - Example:
>
        let g:ollama_debug = 4

2. g:ollama_logfile
    - Description: Configure the path of the log file.
      Note that the log file path of the python code is currently hardcoded to
      '/tmp/logs'.
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama.log'
    - Example:
>
        let g:ollama_logfile = '/path/to/vim-ollama.log'

3. g:ollama_review_logfile
    - Description: Configure the path for logging chat conversations.
      This only will be logged on _Debug_ level (4).
    - Default: A random file in your temp directory will be created using
      tempname(). This can result in a path like '/tmp/vBu3NCc/5-ollama-review.log'
    - Example:
>
        let g:ollama_review_logfile = '/path/to/vim-ollama-review.log'

==============================================================================
8. License                                            *vim-ollama-license*

vim-ollama is open-source software licensed under the GPLv3 License. For more
information, see the LICENSE file in the repository.

==============================================================================
vim:tw=78:et:ft=help:norl:

